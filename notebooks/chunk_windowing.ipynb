{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Windowing\n",
    "\n",
    "Once you've matched the top n similar chunks to a query, you could just put each chunk by itself into context. This will work ok, but can we do better? Maybe you could put whatever document the chunk came from entirely into context. This is better than supplying the chunk in isolation and works well if all the documents are consistent in size and relatively small. But what if there are really large documents in the corpus? You could end up sending a bunch of irrelevant context to the LLM. Maybe this is somewhat ok too, but it comes with additional latency and cost. If you're looking for a middle ground, you could try chunk windowing.\n",
    "\n",
    "<!-- more -->\n",
    "\n",
    "Chunk windowing is pretty straightforward. Choose a window size. Don't overthink it. We are just trying to find a good middle ground between sending too much context (the whole document) and too little (a single chunk). Four is a good default. Just fetch 2 chunks before and one after the current chunk for a total of 4 chunks. Concatenate them and provide it all together in context. It's that simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_slice(chunks, match_index, n: int = 4):\n",
    "    half_window = n // 2\n",
    "    start = max(0, match_index - half_window)\n",
    "    end = min(len(chunks), start + n)\n",
    "    if end == len(chunks):\n",
    "        start = max(0, end - n)\n",
    "    print(f\"start: {start}, end: {end}\")\n",
    "    return chunks[start:end]\n",
    "\n",
    "\n",
    "document_chunks = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 0, end: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_slice(document_chunks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2, end: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C', 'D', 'E', 'F']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_slice(document_chunks, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 4, end: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E', 'F', 'G', 'H']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_slice(document_chunks, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-GTV1m9T0-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
